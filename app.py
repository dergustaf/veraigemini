import streamlit as st
import os
import google.generativeai as genai
from openai import OpenAI
from pinecone import Pinecone

# --- 1. CONFIGURATION ---
# We load keys from Streamlit Secrets
try:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
    PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (FileNotFoundError, KeyError):
    # If running locally without secrets.toml, you might want to hardcode keys here for testing
    # But for deployment, Secrets are best.
    pass

# Configure Pinecone
INDEX_HOST = "veraibot1536-o0tqsfu.svc.aped-4627-b74a.pinecone.io"
NAMESPACES = ["book-mybook-cs", "blog-cs", "podcast_cs"]

# --- 2. SETUP PAGE ---
st.set_page_config(page_title="AI Kou캜 V캩ra (Gemini)", page_icon="游꺔")
st.title("游꺔 AI Kou캜 (V캩ra Svach)")
st.markdown("Zeptejte se na cokoliv ohledn캩 seberozvoje, stresu nebo mindfulness.")

# --- 3. INITIALIZE CLIENTS ---
@st.cache_resource
def init_clients():
    if "OPENAI_API_KEY" not in os.environ:
        st.error("Chyb칤 API kl칤캜e. Zkontrolujte nastaven칤 Secrets.")
        st.stop()
    
    # Configure Google
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    
    # Return OpenAI (for embeddings) and Pinecone
    return OpenAI(), Pinecone(api_key=PINECONE_API_KEY)

client, pc = init_clients()
index = pc.Index(host=INDEX_HOST)

# --- 4. ROBUST MODEL SELECTOR ---
def get_gemini_model():
    """
    Automatically finds a working model (Flash or Pro) to avoid 404 errors.
    """
    try:
        # Ask Google which models are available for this key
        available = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        # Preferred order: Flash -> 1.5 Pro -> Old Pro
        preferences = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-pro']
        
        for pref in preferences:
            if pref in available:
                # Remove 'models/' prefix for initialization
                clean_name = pref.replace("models/", "")
                return genai.GenerativeModel(clean_name)
        
        # Fallback: Just try 'gemini-pro' if list lookup fails logic
        return genai.GenerativeModel('gemini-pro')
        
    except Exception:
        # Ultimate fallback
        return genai.GenerativeModel('gemini-pro')

def get_embedding(text):
    text = text.replace("\n", " ")
    response = client.embeddings.create(
        input=[text], model="text-embedding-3-small"
    )
    return response.data[0].embedding

def retrieve_context(query):
    try:
        query_vector = get_embedding(query)
    except Exception:
        return "", ""

    all_matches = []
    for ns in NAMESPACES:
        try:
            results = index.query(
                namespace=ns, vector=query_vector, top_k=3, include_metadata=True
            )
            for match in results['matches']:
                match['source_namespace'] = ns
                all_matches.append(match)
        except Exception:
            pass

    sorted_matches = sorted(all_matches, key=lambda x: x['score'], reverse=True)
    
    contexts = []
    debug_text = "" 
    
    for match in sorted_matches[:5]:
        text_content = match['metadata'].get('text', '')
        source = match.get('source_namespace', 'unknown')
        score = match.get('score', 0)
        
        if text_content:
            contexts.append(text_content)
            debug_text += f"--- [Zdroj: {source} | Relevence: {score:.2f}] ---\n{text_content[:300]}...\n\n"
            
    return "\n\n".join(contexts), debug_text

def get_response(user_input, chat_history):
    # 1. Retrieve Context
    context, debug_text = retrieve_context(user_input)
    
    if not context:
        context_message = "V datab치zi jsem nena코la p콏칤mou odpov캩캞."
    else:
        context_message = context

    # 2. Prepare History for Gemini
    # Gemini uses 'user' and 'model' roles. Streamlit uses 'user' and 'assistant'.
    gemini_history = []
    for msg in chat_history[:-1]: # Skip the very last message (current prompt) to avoid duplication
        role = "user" if msg["role"] == "user" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})

    # 3. Setup Model & Chat
    model = get_gemini_model()
    chat = model.start_chat(history=gemini_history)
    
    # 4. Create the System Prompt + Query
    # We inject the Persona and Context into the message itself
    system_instruction = """
    Jsi AI kou캜 zalo쬰n칳 na filozofii V캩ry Svach.
    Bu캞 empatick치, stru캜n치 a mluv 캜esky.
    """
    
    final_prompt = f"""
    {system_instruction}
    
    KONTEXT Z DATAB츼ZE (pro aktu치ln칤 ot치zku):
    {context_message}
    
    OT츼ZKA U콯IVATELE:
    {user_input}
    
    Odpov캩z prim치rn캩 na z치klad캩 kontextu v칳코e. Pokud u쬴vatel navazuje na p콏edchoz칤 konverzaci (nap콏. 'ona', 'to'), pou쬴j historii chatu.
    """
    
    try:
        response = chat.send_message(final_prompt)
        return response.text, debug_text
    except Exception as e:
        return f"Chyba Gemini: {str(e)}", ""

# --- 5. CHAT INTERFACE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message:
             with st.expander("游댌 Zobrazit zdroje (Historie)"):
                st.text(message["sources"])

# User Input
if prompt := st.chat_input("Napi코te svou ot치zku..."):
    # 1. Show User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Generate Answer
    with st.chat_message("assistant"):
        with st.spinner("Hled치m v datab치zi a p콏em칳코l칤m (Gemini)..."):
            response_text, sources_text = get_response(prompt, st.session_state.messages)
            st.markdown(response_text)
            
            with st.expander("游댌 Zobrazit pou쬴t칠 texty (D콢kaz)"):
                st.text(sources_text)
            
    # 3. Save Assistant Message
    st.session_state.messages.append({
        "role": "assistant", 
        "content": response_text,
        "sources": sources_text
    })
